---
title: "Using the `greta` package for Dynamic Factor Analysis"
output:
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
    toc_depth: 3
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[__Mark D. Scheuerell__](https://faculty.washington.edu/scheuerl/)  
_Fish Ecology Division, Northwest Fisheries Science Center, National Marine Fisheries Service, National Oceanic and Atmospheric Administration, Seattle, WA USA, mark.scheuerell@noaa.gov_

## Background

Dynamic Factor Analysis (DFA) is a dimension reduction technique specific to time series analysis. The general idea is to model $N$ time series as a linear combination of $M$ "hidden" temporal trends, where $N >> M$. For an $N \times T$ matrix $\mathbf{y}$, the DFA model is

$$
\begin{gathered}
\mathbf{y}_t = \mathbf{Z}\mathbf{x}_t+\mathbf{a}+\mathbf{v}_t \\
\mathbf{x}_t = \mathbf{x}_{t-1}+\mathbf{w}_t
\end{gathered}   
$$

The matrix $\mathbf{Z}$ maps the latent trends onto the observed data. The vector $\mathbf{a}$ contains time series specific offsets. The observation errors are mulitvaraite normal, such that $\mathbf{v}_t \sim \text{MVN}(\mathbf{0},\mathbf{R})$. The form for $\mathbf{R}$ can be anything from simple IID errors (i.e., same variance parameter down the diagonal ans zeroes elsewhere), to something much more complex like an unconstrained block diagonal. For convenience, the variance-covariance matrix $\mathbf{Q}$ of the process errors, where $\mathbf{w}_t \sim \text{MVN}(\mathbf{0},\mathbf{Q})$ governing the multivariate random walk is generally assumed to be an $M \times M$ identity matrix, $\mathbf{I}_M$.

## Why `greta`?

Estimating the parameters in a DFA model is not trivial. There are several available methods for doing so, including an EM algorithm as implemented in the [`MARSS`](https://cran.r-project.org/web/packages/MARSS/index.html) package, or an MCMC algorithm as implemented in the __Stan__ language via the [`statss`](https://github.com/nwfsc-timeseries/statss) package. However, the fitting process can be extremely slow when working with really large data sets (e.g., 50+ time series).

For example, a couple of years ago I helped a graduate student with a [DFA analysis](https://doi.org/10.1371/journal.pone.0162121) based on daily growth rates of juvenile salmon. She had 100+ time series from four different year/month combinations, each of which was about 40-180 days long. We fit our models with `MARSS` using a distributed cluster from [Domino](https://www.dominodatalab.com/), but the models still took days to converge.

The [`greta`](https://github.com/goldingn/greta) package was designed by [Nick Golding](https://scholar.google.co.uk/citations?user=peoal7wAAAAJ&hl=en) to do [Hamiltonian Monte Carlo](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12681/full) via Google's [TensorFlow](https://www.tensorflow.org/) computational engine, which makes it really fast on big datasets. It's also nice because you write the models using normal `R` syntax.

## Installation

You can install `greta` from GitHub using `devtools`

```{r, eval=FALSE, message=FALSE}
devtools::install_github('goldingn/greta')
```

__NOTE__: You will also need to install TensorFlow (version 1.0.0 or higher) before `greta` will work. Check [here](https://www.tensorflow.org/install/) for instructions.

## Example

Here I'll create a large number of time series that share some interdependencies (i.e., common trends) among them, and then I'll see how well `greta` does at recovering the original parameters. To begin, I'll use 100 time series that are 50 units long, each of which is a linear combination of 5 different latent trends.

```{r data_inits}
NN <- 100
TT <- 50
MM <- 5
```

### Simulate latent trends

Next I'll simulate the latent trends. In this case, each of them is random walk, so it's easy to just work with the innovations. To do so, I make use of `mvrnorm` function in the `MASS` package. I also make use of the color palettes in `viridis`.

```{r load_pkgs}
library(MASS)
library(viridis)
```

I will discard the first `r TT` samples to eliminate the effects of the initial starting conditions.

```{r sim_trends, fig.height=4, fig.width=7, message=FALSE}
set.seed(1234)
## MM x TT matrix of innovations
ww <- t(mvrnorm(TT*2, matrix(0,MM,1), diag(MM)))
## MM x TT matrix of latent trends
xx <- t(apply(ww[,-(1:TT)],1,cumsum))
## plot the trends
par(mai=c(0.8,0.8,0.2,0.2))
matplot(t(xx), type="b", lty="solid", cex=0.8,
        xlab="Time", ylab=expression(italic(x)[italic(t)]),
        col=plasma(MM, end=0.8))
```

### Create loadings matrix

The next thing to do is create the matrix $\mathbf{Z}$ that maps the trends onto the observations. I want some of the time series to look more like one another than others, so I'll use a range of values within some blocks.

```{r create_Z}
ZZ <- matrix(NA, NN, MM)
LL <- seq(-1,1,length.out=MM+1)
for(i in 1:MM) {
  zz <- NULL
  for(j in 1:(MM)) {
    zz <- c(zz,runif(NN/MM,LL[j],LL[j+1]))
  }
  ZZ[sample(NN,NN),i] <- round(zz,3)
}
```

### Create observation errors

Now I need to create the observation errors to add to the time series. For this first test, I'll assume that the errors are IID, and their variance is 0.2.

```{r create_obs_errs}
vv <- t(mvrnorm(TT, matrix(0,NN,1), diag(0.2,NN,NN)))
```

### Create observed time series

Now I can use the loadings matrix and observation errors to create the observed time series. For now I will ignore the additive effect of the offset vector $\mathbf{a}$.

```{r create_ts, fig.height=4, fig.width=7}
yy <- ZZ %*% xx + vv
## plot the trends
par(mai=c(0.8,0.8,0.2,0.2))
matplot(t(yy), type="l", lty="solid",
        xlab="Time", ylab=expression(italic(y)[italic(t)]),
        col=plasma(NN, alpha=0.7, end=0.8))
```

Although many of the time series in this collection of `r NN` time series do not appear to be highly correlated, some of them are by definition. Here is a histogram of the correlation coefficients for all of the pairwise comparisons. 

```{r hist_cor_coefs, fig.height=4, fig.width=7}
rho <- cor(yy)
par(mai=c(0.8,0.8,0.2,0.2))
hist(rho[lower.tri(rho)], breaks=seq(-10,10)/10, col=viridis(1,begin=0.4,end=0.4),
     xlab="Correlation", ylab="", main="")
```

## Parameter estimation

One of the advantages of `greta` is that the model syntax looks just like native `R`. The basic container for data and parameters is the `greta_array`.

```{r load_greta, message=FALSE}
library(greta)
```

### Specify priors

The first thing to do is specify the prior distributions for the model parameters. Those for $\mathbf{x}_0$ (i.e., the state at time 0) and $\mathbf{R}$ are straightforward.

```{r priors_1}
## initial state
X0 <- normal(0,10,MM)
## var-cov matrix of obs errors
RR_est <- zeros(NN,NN)
for(i in 1:NN) {
  RR_est[i,i] <- normal(0,10)
}
```

However, the specification of $\mathbf{Z}$ is not as simple. To make the model identifiable when $M > 1$, the upper right section of $\mathbf{Z}$ must be set equal to zero. For example, if $M = 3$ and $N = 5$, then 

$$
\mathbf{Z} = 
\begin{bmatrix}
    z_{11} & 0      & 0 \\
    z_{21} & z_{22} & 0 \\
    z_{31} & z_{32} & z_{33} \\
    z_{41} & z_{42} & z_{43} \\
    z_{51} & z_{52} & z_{53}
\end{bmatrix}   
$$

Thus, we need to tell `greta` which of the parameters in $\mathbf{Z}$ are to be estimated, and which are set to 0. The order of operations is important here: `greta` does not allow you to reassign elements in an array to fixed values after they have already been declared as random.

```{r priors_2}
## loadings matrix
ZZ_est <- zeros(NN,MM)
for(i in 1:MM) {
  ZZ_est[i:NN,i] <- normal(0,10,NN-i+1)
}
```

### Specify likelihoods

The next step is to set up the likelihood functions. To do so, `greta` requires a specification of the expectation for the mean and variance of the multivariate normal distributions. Specifically, in the DFA model the of interest distributions are

$$
\begin{gathered}
\mathbf{y}_t \sim \text{MVN}(\mathbf{Z}\mathbf{x}_t,\mathbf{R}) \\
\mathbf{x}_t \sim \text{MVN}(\mathbf{x}_{t-1},\mathbf{Q}) \\
\mathbf{x}_0 \sim \text{MVN}(\mathbf{0},\mathbf{\infty})
\end{gathered}
$$ 

