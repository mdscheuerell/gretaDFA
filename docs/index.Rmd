---
title: "Dynamic Factor Analysis with the __greta__ package for __R__"
output:
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
    toc_depth: 3
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tensorflow.one_based_extract = TRUE)
```

[__Mark D. Scheuerell__](https://faculty.washington.edu/scheuerl/)  
_Northwest Fisheries Science Center, National Oceanic and Atmospheric Administration, Seattle, WA USA_

[__Nick Golding__](https://qaeco.com/researchfellows/#ngolding)  
_Quantitative and Applied Ecology Group, School of BioSciences, University of Melbourne, Melbourne, AUS_

***

__DISCLAIMER__  

This vignette is still in the testing and evaluating phase and should not be considered complete or error-free.

This is version `r paste0('0.',format(Sys.time(), '%y.%m.%d'))`.

***

# Background

Dynamic Factor Analysis (DFA) is a dimension reduction technique specific to time series analysis. The general idea is to model $N$ time series as a linear combination of $M$ "hidden", time-varying factors, where $M \ll N$. For an $N \times T$ matrix of data $\mathbf{y}$, where $\mathbf{y}_t$ is an $N \times 1$ column vector, the DFA model is

$$
\mathbf{y}_t \sim \text{MVN}(\mathbf{Z} \mathbf{x}_t + \mathbf{a},\mathbf{R})
$$
$$
\mathbf{x}_t \sim \text{MVN}(\mathbf{x}_{t-1},\mathbf{Q})
$$
$$
\mathbf{x}_0 \sim \text{MVN}(\mathbf{0},\mathbf{Q}_0)
$$

The $N \times M$ matrix $\mathbf{Z}$ maps the factors onto the observed data at time $t$. The $N \times 1$ vector $\mathbf{a}$ contains offsets for each of the observed time series. The covariance matrix $\mathbf{R}$ of the observation errors can be anything from simple IID errors (i.e., same variance parameter down the diagonal ans zeroes elsewhere), to something much more complex like an unconstrained block diagonal.

The factors are modeled as random walks where the covariance matrix $\mathbf{Q}$ of the process errors governing their evolution is generally assumed to be an $M \times M$ identity matrix, $\mathbf{I}_M$. The covariance matrix $\mathbf{Q}_0$ of the initial states $\mathbf{x}_0$ is typically assumed to have large values along the diagonal and zeros elsewhere.

# Why __greta__?

Estimating the parameters in a DFA model is not trivial. There are several available methods for doing so, including an EM algorithm as implemented in the [__MARSS__](https://cran.r-project.org/web/packages/MARSS/index.html) package. However, the fitting process can be extremely slow when working with really large data sets (e.g., 50+ time series).

For example, a couple of years ago MDS helped a graduate student with a [DFA analysis](https://doi.org/10.1371/journal.pone.0162121) based on estimates of daily growth rates for juvenile Chinook salmon. She had 100+ time series from four different year/month combinations, each of which was about 40-180 days long. We fit our models with __MARSS__ using a distributed cluster from [Domino](https://www.dominodatalab.com/), but the models still took days to converge.

The [__greta__](https://greta-dev.github.io/greta/) package was designed by NG to do [Hamiltonian Monte Carlo](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12681/full) via Google's [TensorFlow](https://www.tensorflow.org/) computational engine, which makes it really fast on big datasets. It's also nice because you write the models using normal __R__ syntax.

# Requirements

For this vignette we're using the development version of __greta__, which you can install from GitHub via. 

```{r, eval=FALSE, message=FALSE}
devtools::install_github('goldingn/greta@dev')
```

You will also need to install [TensorFlow](https://www.tensorflow.org/install/) and the [tensorflow](https://cran.r-project.org/web/packages/tensorflow/) package before __greta__ will work. See the output below for the versions used here.

```{r load_pkgs_1, message=FALSE}
library(tensorflow)
library(greta)
installed.packages()[c("greta", "tensorflow"), "Version"]
tensorflow::tf_version()
```

In addition, we also make use of `mvrnorm()` function in the __MASS__ package and the color palettes in the __viridis__ package. 

```{r load_pkgs_2, message=FALSE}
library(MASS)
library(viridis)
```

# Simulate data

Here we'll create a large number of time series that share some interdependencies (i.e., common trends) among them, and then we'll see how well __greta__ does at recovering the original parameters. We'll use 100 time series that are 50 units long, each of which is a linear combination of 5 different latent trends.

```{r data_inits}
NN <- 50
TT <- 30
MM <- 5
```

## Latent factors

We begin with the latent factors in $\mathbf{x}$. In this case, each of the rows in $\mathbf{x}$ is an independent random walk, so it's easy to just work with the process errors. Note that after simulating the latent trends, we subtract the mean of each so we can avoid estimating the offsets in the vector $\mathbf{a}$.

```{r sim_factors, message=FALSE}
set.seed(123)
## MM x TT matrix of innovations
ww <- matrix(rnorm(MM*TT, 0, 1), MM, TT)
## MM x TT matrix of latent trends
xx <- t(apply(ww,1,cumsum))
## convert to mean-zero
xx <- xx - apply(xx,1,mean)
```

```{r plot_factors, fig.height=4, fig.width=7, echo=FALSE}
## plot the trends
par(mai=c(0.8,0.8,0.2,0.2))
matplot(t(xx), type="b", lty="solid", cex=0.7,
        xlab="Time", ylab=expression(italic(x)[italic(t)]),
        col=plasma(MM, end=0.8))
```

## Loadings matrix

The next thing to do is create the matrix $\mathbf{Z}$ that maps the trends onto the observations. We want some of the time series to look more like one another than others, so we'll randomly draw the loadings from a uniform range of values within specific blocks. (Note that this bit of code only works if `NN %% MM == 0`.)

```{r create_Z}
## NN x MM loadings matrix
ZZ <- matrix(NA, NN, MM)
LL <- seq(-1,1,length.out=MM+1)
for(i in 1:MM) {
  zz <- NULL
  for(j in 1:MM) {
    zz <- c(zz,runif(NN/MM,LL[j],LL[j+1]))
  }
  ZZ[sample(NN,NN),i] <- round(zz,3)
}
```

## Observed time series

Now we can use the loadings matrix and some observation errors to create the observed time series. Here we'll assume that the errors are IID, and their variance is 0.2. We can ignore the additive effect of the offset vector $\mathbf{a}$ because the expectations of $\mathbf{x}$ and $\mathbf{e}$ are both zero.

```{r create_ts}
## obs errors
ee <- t(mvrnorm(TT, matrix(0,NN,1), diag(0.2,NN,NN)))
## NN x TT matrix of observed data
yy <- ZZ %*% xx + ee
```

```{r plot_ts, fig.height=4, fig.width=7, echo=FALSE}
par(mai=c(0.8,0.8,0.2,0.2))
matplot(t(yy), type="l", lty="solid",
        xlab="Time", ylab=expression(italic(y)[italic(i)]),
        col=plasma(NN, alpha=0.7, end=0.8))
```

It's hard to tell from this plot how many of the `r NN` time series are correlated. Here is a histogram of the correlation coefficients for all of the pairwise comparisons. 

```{r hist_cor_coefs, fig.height=4, fig.width=7, echo=FALSE}
rho <- cor(t(yy))
par(mai=c(0.8,0.8,0.2,0.2))
hist(rho[lower.tri(rho)], breaks=seq(-10,10)/10, freq=FALSE,
     col=plasma(1,begin=0.5,end=0.5), border="white",
     xlab="Correlation", ylab="", main="")
```

# Parameter estimation

We begin by defining a function that returns the cumulative sum of the rows (or columns) of a matrix that we will use for estimating the latent factors $(\mathbf{x})$.

```{r load_greta, message=FALSE}
op <- greta::.internals$nodes$constructors$op
cumsum_mat <- function(x, a=1L) {
  ## cumsums across matrix rows/cols
  ## rows: a = 1L
  ## cols: a = 0L
  stopifnot(length(dim(x)) == 2)
  op("cumsum_mat",
     x,
     tf_operation = tf$cumsum,
     operation_args = list(axis = a))
}
```

## Specify priors

### Loadings matrix

To make the model identifiable when $M$ > 1, the upper right section of $\mathbf{Z}$ must be set equal to zero. For example, if $M$ = 4 then 

$$
\mathbf{Z} = 
\begin{bmatrix}
    z_{11} & 0      & 0 & 0 \\
    z_{21} & z_{22} & 0 & 0 \\
    z_{31} & z_{32} & z_{33} & 0 \\
    z_{41} & z_{42} & z_{43} & z_{44} \\
    z_{51} & z_{52} & z_{53} & z_{54} \\
    \vdots & \vdots & \vdots & \vdots \\
    z_{N1} & z_{N2} & z_{N3} & z_{N4} 
\end{bmatrix}.   
$$

Thus, we need to tell __greta__ which of the parameters in $\mathbf{Z}$ are to be estimated, and which should be fixed at 0. The order of operations is important here because __greta__ does not allow you to reassign elements in an array to fixed values after they have already been declared as random.

```{r priors_loadings}
## loadings matrix
ZZ_est <- zeros(NN,MM)
idx <- lower.tri(ZZ_est, diag = TRUE)
ZZ_est_raw <- normal(0, 10, dim = sum(idx))
ZZ_est[idx] <- ZZ_est_raw
```

The reason we defined `ZZ_est_raw` separately here, rather than directly assigning values to `ZZ_est` is so that we can use the `evaluate()` function to verify its contents. With `evaluate()`, you can calculate the value of a greta array by passing a fixed value for those it depends on.

So, for example, if we specified $\mathbf{Z}$ correctly, the following should return the top portion of a matrix with a 1 for those elements that are to be estimated and a 0 otherwise.

```{r demo_evaluate, cache=FALSE}
head(evaluate(ZZ_est, list(ZZ_est_raw = rep(1, sum(idx)))))
```

It looks like everything worked as expected.

### Covariance matrix

Defining the priors for the covariance matrix $\mathbf{R}$ is straightforward. In this case, there is no covariance among the observation errors so we could model them as a set of univariate normals, but we will keep the code general enough to allow for possible covariance(s).

The maximum variance among all of the observed time series is about `r round(max(apply(yy, 1, var)),)` so we'll use a relatively uninformative half-cauchy prior on the variances with a scale of 15 and truncate the upper end at 15. 

```{r priors_obs_cov}
## diagonal of R
RR_est_raw <- cauchy(0, 15, 1, truncation=c(0, 15))
```

## Specify likelihoods

The next step is to specify the likelihood functions.

### Factors

For the factor model, we need a way to estimate the temporal evolution of the random walks. This is where we can take advantage of the `cumsum_mat()` function we defined above. Also recall that we defined the covariance matrix $\mathbf{Q}$ to be the identity matrix $\mathbf{I}$.

```{r like_factors}
## inital factor
X0 = normal(0, 10, c(MM, 1))
## factors 2-TT
XX = normal(0, 1, c(MM, TT))
## combine & cumsum over rows for the random walk
xx_est <- cumsum_mat(cbind(X0,XX))[,-1]
```

### Observed data

Here __greta__ requires a specification of the expectation for the mean and variance of the multivariate normal distributions. However, rather than draw time-specific samples we will instead 1) create $NT \times 1$ vectors of the data $\mathbf{y}$ and expectation $\mathbf{Z} \mathbf{x}$; and 2) create an $NT \times NT$ covariance matrix based on $T$ replicates of $\mathbf{R}$ along the diagonal.

```{r like_obs, eval=TRUE, echo=TRUE}
## vectorize data
yy_vec <- as_data(matrix(yy, 1, NN*TT))
## vectorize mean
Zx_vec <- c(ZZ_est %*% xx_est)
## create expanded cov matrix
RR_star <- zeros(NN*TT, NN*TT)
diag(RR_star) <- rep(RR_est_raw, NN*TT)
## define likelihood
distribution(yy_vec) = multivariate_normal(Zx_vec, RR_star)
```

## Fit the model

Now we can define the complete model and do some MCMC sampling.

```{r greta_model, cache=TRUE}
mod_fit <- model(xx_est, ZZ_est, RR_est_raw)
mcmc_smpl <- mcmc(mod_fit, n_samples = 2000, thin = 1, warmup = 1000,
                  chains = 1, verbose = FALSE)
```

# Examining the fits

We begin by summarizing the MCMC samples.

```{r examine_fits}
mod_smry <- summary(mcmc_smpl)
par_means <- mod_smry$statistics[,"Mean"]
```

## Loadings matrix

Recall how we arbitrarily constrained $\mathbf{Z}$ in such a way to choose only one of many possible solutions, but fortunately the different solutions are equivalent, and they can be related to each other by a rotation matrix $\mathbf{H}$.  Let $\mathbf{H}$ be any $m \times m$ non-singular matrix.  The following are then equivalent DFA models:

$$
\begin{gathered}
 \mathbf{x}_t = \mathbf{x}_{t-1}+\mathbf{w}_t \\
 \mathbf{y}_t = \mathbf{Z}\mathbf{x}_t+\mathbf{a}+\mathbf{v}_t
\end{gathered}   
$$

and

$$
\begin{gathered}
 \mathbf{H}\mathbf{x}_t = \mathbf{H}\mathbf{x}_{t-1}+\mathbf{H}\mathbf{w}_t \\
 \mathbf{y}_t = \mathbf{Z}\mathbf{H}^{-1}\mathbf{x}_t+\mathbf{a}+\mathbf{v}_t
\end{gathered}.
$$

There are many ways of doing factor rotations, but a common method is the "varimax"" rotation, which seeks a rotation matrix $\mathbf{H}$ that creates the largest difference between the loadings in $\mathbf{Z}$.
 
The varimax rotation is easy to compute because R has a built in function for this: `varimax()`. Interestingly, the function returns the inverse of $\mathbf{H}$, which we need anyway.  

```{r get_ZZ_fit}
ZZ_fit <- par_means[grepl("ZZ",rownames(mod_smry$statistics))]
ZZ_fit <- matrix(ZZ_fit, NN, MM, byrow=FALSE)
## rotation matrix
HH_inv <- varimax(ZZ_fit)$rotmat
## rotated Z
ZZ_rot <- ZZ_fit %*% HH_inv
round(ZZ_rot, 2)
```

## Factors

Here are the `r MM` factors in $\mathbf{x}$. Note that we need to rotate these just as we did $\mathbf{Z}$. The top panel has the true factors; the bottom panel shows the estimated factors. Note that there is no way to insure that the specific ordering of the estimated factors will match the true factors.

```{r get_factors}
# fitted factors
xx_fit <- par_means[grepl("xx",rownames(mod_smry$statistics))]
xx_fit <- matrix(xx_fit, MM, TT, byrow=FALSE)
## rotated factors
xx_rot <- solve(HH_inv) %*% xx_fit
```

```{r plot_xx_fits, fig.height=7, fig.width=7, echo=FALSE}
par(mfrow=c(2,1), mai=c(0.8,0.8,0.2,0.2))
matplot(t(xx), type="b", lty="solid", cex=0.7,
        xlab="Time", ylab=expression(italic(x)[italic(t)]),
        col=plasma(MM, end=0.8))
matplot(t(xx_rot), type="l", lty="solid", cex=0.7,
        xlab="Time", ylab=expression(italic(x)[italic(t)]),
        col="darkgray")
```

## Covariance matrix

and the covariance matrix $\mathbf{R}$.

```{r get_RR_fit}
RR_fit <- par_means[grepl("RR",rownames(mod_smry$statistics))]
RR_fit <- diag(RR_fit)
round(RR_fit, 2)
```

## Expectations

Here are the expectations.

```{r get_yy_fits}
yy_fit <- ZZ_rot %*% xx_rot
```

```{r plot_yy_fit, fig.height=4, fig.width=7, echo=FALSE}
par(mai=c(0.8,0.8,0.2,0.2))
matplot(t(yy), type="l", lty="solid",
        xlab="Time", ylab=expression(italic(y)[italic(t)]),
        col=plasma(NN, alpha=0.7, end=0.8))
matlines(t(yy_fit), lty="dashed",
         col=plasma(NN, alpha=0.7, end=0.8))
```

```{r bayes_plots}
#bayesplot::mcmc_trace(mcmc_smpl)
```