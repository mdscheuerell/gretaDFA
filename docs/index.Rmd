---
title: "Dynamic Factor Analysis with the __greta__ package for __R__"
output:
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
    toc_depth: 3
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tensorflow.one_based_extract = TRUE)
```

<br/>

[__Mark D. Scheuerell__](https://faculty.washington.edu/scheuerl/)  
_Northwest Fisheries Science Center, National Oceanic and Atmospheric Administration, Seattle, WA USA_

[__Nick Golding__](https://qaeco.com/researchfellows/#ngolding)  
_Quantitative and Applied Ecology Group, School of BioSciences, University of Melbourne, Melbourne, Victoria, AUS_

***

This is version `r paste0('0.',format(Sys.time(), '%y.%m.%d'))`.

***

__DISCLAIMER__  

This vignette is still in the testing and evaluating phase and should not be considered complete or error-free.

# Background

Dynamic Factor Analysis (DFA) is a dimension reduction technique specific to time series analysis. The general idea is to model $N$ time series as a linear combination of $M$ "hidden", time-varying factors, where $M \ll N$. For an $N \times T$ matrix of data $\mathbf{y}$, where $\mathbf{y}_t$ is an $N \times 1$ column vector, the DFA model is

$$
\mathbf{y}_t \sim \text{MVN}(\mathbf{Z} \mathbf{x}_t + \mathbf{a},\mathbf{R})
$$
$$
\mathbf{x}_t \sim \text{MVN}(\mathbf{x}_{t-1},\mathbf{Q})
$$
$$
\mathbf{x}_0 \sim \text{MVN}(\mathbf{0},\mathbf{Q}_0)
$$

The $N \times M$ matrix $\mathbf{Z}$ maps the factors onto the observed data at time $t$. The $N \times 1$ vector $\mathbf{a}$ contains offsets for each of the observed time series. The covariance matrix $\mathbf{R}$ of the observation errors can be anything from simple IID errors (i.e., same variance parameter down the diagonal ans zeroes elsewhere), to something much more complex like an unconstrained block diagonal.

The factors are modeled as random walks where the covariance matrix $\mathbf{Q}$ of the process errors governing their evolution is generally assumed to be an $M \times M$ identity matrix, $\mathbf{I}_M$. The covariance matrix $\mathbf{Q}_0$ of the initial states $\mathbf{x}_0$ is typically assumed to have large values along the diagonal and zeros elsewhere.

# Why __greta__?

Estimating the parameters in a DFA model is not trivial. There are several available methods for doing so, including an EM algorithm as implemented in the [__MARSS__](https://cran.r-project.org/web/packages/MARSS/index.html) package. However, the fitting process can be extremely slow when working with really large data sets (e.g., 50+ time series).

For example, a couple of years ago MDS helped a graduate student with a [DFA analysis](https://doi.org/10.1371/journal.pone.0162121) based on estimates of daily growth rates for juvenile Chinook salmon. She had 100+ time series from four different year/month combinations, each of which was about 40-180 days long. We fit our models with __MARSS__ using a distributed cluster from [Domino](https://www.dominodatalab.com/), but the models still took days to converge.

The [__greta__](https://greta-dev.github.io/greta/) package was designed by NG to do [Hamiltonian Monte Carlo](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12681/full) via Google's [TensorFlow](https://www.tensorflow.org/) computational engine, which makes it really fast on big datasets. It's also nice because you write the models using normal __R__ syntax.

# Requirements

For this vignette we're using the development version of __greta__, which you can install from GitHub via 

```{r, eval=FALSE, message=FALSE}
devtools::install_github('goldingn/greta@dev')
```

You will also need to install [TensorFlow](https://www.tensorflow.org/install/) and the [tensorflow](https://cran.r-project.org/web/packages/tensorflow/) package for __R__ before __greta__ will work. See the output below for the versions used here.

```{r load_pkgs_1, message=FALSE}
library(tensorflow)
library(greta)
installed.packages()[c("greta", "tensorflow"), "Version"]
tensorflow::tf_version()
```

In addition, we also make use of the `mvrnorm()` function in the __MASS__ package, color palettes in the __viridis__ package, and the `corrplot()` function from the __corrplot__ package. 

```{r load_pkgs_2, message=FALSE}
library(MASS)
library(viridis)
library(corrplot)
```

Lastly, we need to define a function that returns the cumulative sum of the rows (or columns) of a matrix that we will use for estimating the latent factors $(\mathbf{x})$.

```{r load_greta, message=FALSE}
op <- greta::.internals$nodes$constructors$op
cumsum_mat <- function(x, a=1L) {
  ## cumsums across matrix rows/cols
  ## rows: a = 1L
  ## cols: a = 0L
  stopifnot(length(dim(x)) == 2)
  op("cumsum_mat",
     x,
     tf_operation = tf$cumsum,
     operation_args = list(axis = a))
}
```


# Simulate data

Our general approach is to create a large number of time series, each of which to a greater or lesser degree share some temporal patterns with one another. We'll use 50 time series that are 30 units long, each of which is a linear combination of 5 different latent trends.

```{r data_inits}
NN <- 50
TT <- 30
MM <- 5
```

## Latent factors

In a DFA model, the rows in the matrix of latent factors $\mathbf{x}$ are generally assumed to be independent random walks, each of which is a cumulative sum of a sequence of independent process errors.

```{r sim_factors, message=FALSE}
set.seed(123)
## MM x TT matrix of innovations
ww <- matrix(rnorm(MM*TT, 0, 1), MM, TT)
ww[,1] <- rnorm(MM, 0, sqrt(5))
## MM x TT matrix of latent trends
xx <- t(apply(ww,1,cumsum))
```

```{r plot_factors, fig.height=4, fig.width=7, echo=FALSE}
## plot the trends
par(mai=c(0.8,0.8,0.2,0.2))
matplot(t(xx), type="b", lty="solid", cex=0.7,
        xlab="Time", ylab=expression(italic(x)[italic(t)]),
        col=plasma(MM, end=0.8))
```

## Loadings matrix

The matrix $\mathbf{Z}$ maps the factors $\mathbf{x}$ onto the observations $\mathbf{y}$. We want some of the time series to look more like one another than others, so we randomly draw the loadings from a uniform range of values within specific blocks. (Note that this bit of code only works if `NN %% MM == 0`.)

```{r create_Z}
## NN x MM loadings matrix
ZZ <- matrix(NA, NN, MM)
LL <- seq(-1,1,length.out=MM+1)
for(i in 1:MM) {
  zz <- NULL
  for(j in 1:MM) {
    zz <- c(zz,runif(NN/MM,LL[j],LL[j+1]))
  }
  ZZ[sample(NN,NN),i] <- round(zz,3)
}
```

## Observed time series

Now we can use the loadings and some observation errors to create the observed time series. Here we assume that the errors are IID, and their variance is 0.2. We can ignore the additive effect of the offset vector $\mathbf{a}$ because the expectations of $\mathbf{x}$ and $\mathbf{e}$ are both zero.

```{r create_ts}
## obs errors
ee <- t(mvrnorm(TT, matrix(0,NN,1), diag(0.2,NN,NN)))
## NN x TT matrix of observed data
yy <- ZZ %*% xx + ee
```

```{r plot_ts, fig.height=4, fig.width=7, echo=FALSE}
par(mai=c(0.8,0.8,0.2,0.2))
matplot(t(yy), type="l", lty="solid",
        xlab="Time", ylab=expression(italic(y)[italic(i)]),
        col=plasma(NN, alpha=0.7, end=0.8))
```

It's hard to tell from this plot how many of the `r NN` time series are correlated. Here is a plot of the correlation coefficients for all of the pairwise comparisons with them clustered by similarity. 

```{r hist_cor_coefs, fig.height=7, fig.width=7, echo=FALSE}
rho <- cor(t(yy))
par(mai=c(0.8,0.8,0.2,0.2))
corrplot(rho, method="ellipse", type="lower", order = "hclust",
         tl.col = "black", tl.srt = 0, tl.cex = 0.6, tl.offset = 0.7,
         cl.cex = 0.8, cl.offset = 0.9, cl.ratio = 0.1)
```

# Parameter estimation

Before we can estimate the model parameters we need to specify distributions for the priors and likelihoods.

## Specify priors

### Loadings matrix

To make the model identifiable when $M$ > 1, we need to impose several constraints on the model. First, the upper right section of $\mathbf{Z}$ must be set equal to zero. For example, if $M$ = 4 then 

$$
\mathbf{Z} = 
\begin{bmatrix}
    z_{11} & 0      & 0 & 0 \\
    z_{21} & z_{22} & 0 & 0 \\
    z_{31} & z_{32} & z_{33} & 0 \\
    z_{41} & z_{42} & z_{43} & z_{44} \\
    z_{51} & z_{52} & z_{53} & z_{54} \\
    \vdots & \vdots & \vdots & \vdots \\
    z_{N1} & z_{N2} & z_{N3} & z_{N4} 
\end{bmatrix}.   
$$

Second, when estimating a DFA model in a Bayesian framework, as we are with __greta__, we need to constrain the diagonal of $\mathbf{Z}$ to be positive to ensure convergence (i.e., $z_{ij} > 0$ $\forall$ $i = j$.

Because we will scale the data below before fitting the model, we can impose some restrictions on the bounds of the priors for the elements in $\mathbf{Z}$. Specifically, via truncated diffuse normal distributions, we will set

$$
z_{ij} \in [0,1] ~ \forall ~ i = j
$$
$$
z_{ij} \in [-1,1] ~ \forall ~ i > j
$$

Thus, we need to tell __greta__ which of the parameters in $\mathbf{Z}$ are to be estimated, and which should be fixed at 0. The order of operations is important here because __greta__ does not allow you to reassign elements in an array to fixed values after they have been declared as random. (Note that we follow the __greta__ convention of using the `=` assignment for stochastic nodes.)

```{r priors_loadings}
## loadings matrix
ZZ_est <- zeros(NN,MM)
## diagonal (must be positive)
idx_d <- row(ZZ_est) == col(ZZ_est)
ZZ_est_raw_d = normal(0, 10, dim = sum(idx_d), truncation = c(0, 1))
ZZ_est[idx_d] <- ZZ_est_raw_d
## sub-diagonal
idx_s <- lower.tri(ZZ_est)
ZZ_est_raw_s = normal(0, 10, dim = sum(idx_s), truncation = c(-1, 1))
ZZ_est[idx_s] <- ZZ_est_raw_s
```

The reason we defined `ZZ_est_raw_` separately here, rather than directly assigning values to `ZZ_est` is so that we can use the `evaluate()` function to verify its contents. With `evaluate()`, we can calculate the value of a greta array by passing a fixed value for those it depends on.

So, for example, if we specified $\mathbf{Z}$ correctly, the following should return the top of `ZZ_est` with a 1 for those elements that are to be estimated and a 0 otherwise.

```{r demo_evaluate, cache=TRUE}
head(calculate(ZZ_est, list(ZZ_est_raw_s = rep(1, sum(idx_s)),
                            ZZ_est_raw_d = rep(1, sum(idx_d)))))
```

It looks like everything worked as expected.

### Covariance matrix

Defining the priors for the covariance matrix $\mathbf{R}$ is straightforward. In this case, there is no covariance among the observation errors so we could model them as a set of univariate normals, but we will keep the code general enough to allow for possible covariance(s).

The maximum variance among all of the observed time series is about `r round(max(apply(yy, 1, var)),)` so we'll use a half-cauchy prior on the observation variance with a scale of 15 and truncate the upper end of the distribution at 15.

```{r priors_obs_cov}
## diagonal of R
RR_est_raw = cauchy(0, 15, 1, truncation=c(0, 5))
```

## Specify likelihoods

The next step is to specify the likelihood functions.

### Factors

For the factor model, we need to specify a model for the temporal evolution of the random walks. This is where we take advantage of the `cumsum_mat()` function we defined above. (Recall that we defined the covariance matrix $\mathbf{Q}$ to be the identity matrix $\mathbf{I}$.)

```{r like_factors}
## inital factor
X0 = normal(0, 10, c(MM, 1))
## factors for t = 2-TT
XX = normal(0, 1, c(MM, TT))
## combine & cumsum over rows for the random walk
xx_est <- cumsum_mat(cbind(X0,XX))[,-1]
```

### Observed data

__greta__ requires a specification for the mean and variance of the multivariate normal distribution. However, rather than draw time-specific samples we will instead 1) create $NT \times 1$ vectors of the data $\mathbf{y}$ and their expectations $\mathbf{Z} \mathbf{x}$; and 2) create an $NT \times NT$ covariance matrix based on $T$ replicates of $\mathbf{R}$ along the diagonal.

Note that we will scale each of the $\mathbf{y}_i$ time series in $\mathbf{y}$ to have unit variance so we won't have to estimate $\mathbf{a}$.

```{r like_obs, eval=TRUE, echo=TRUE}
## scale data
yy_z <- t(scale(t(yy)))
## vectorize data
yy_vec <- as_data(matrix(yy_z, 1, NN*TT))
## vectorize mean
Zx_vec <- c(ZZ_est %*% xx_est)
## create expanded cov matrix
RR_star <- zeros(NN*TT, NN*TT)
diag(RR_star) <- rep(RR_est_raw, NN*TT)
## define likelihood
distribution(yy_vec) = multivariate_normal(Zx_vec, RR_star)
```

## Fit the model

Now we can define the complete model and do some MCMC sampling.

```{r greta_model, cache=TRUE}
mod_fit <- model(xx_est, ZZ_est, RR_est_raw)
mcmc_smpl <- mcmc(mod_fit, n_samples = 5000, thin = 4, warmup = 1000,
                  chains = 1, verbose = FALSE)
```

# Assessing convergence

It is important to determine whether or not the model has converged before we can place any confidence in the results. Among the many diagnostic measures available, the options are somwhat limited when there is only one chain. 

## Traceplots

One quick option is to visually inspect the MCMC traces with `traceplot()` from the __coda__ package. Because the output is so voluminous, we only show them for the first 4 estimates.

```{r traceplots, fig.height=5, fig.width=7}
par(mfrow=c(2,2), mai=c(0.8,0.5,0.2,0.2))
for(i in 1:4) {
  coda::traceplot(mcmc_smpl[[1]][,i]) 
}
```

```{r bayesplot, fig.height=5, fig.width=7, eval=FALSE, echo=FALSE}
par(mai=c(0.8,0.5,0.2,0.2))
bayesplot::mcmc_trace(as.matrix(mcmc_smpl[[1]][,1:4]))
```

These chains clearly show evidence of nonconvergence and also some autocorrelation, suggesting we need to use a longer burnin period and perhaps some thinning as well.

## Formal test

Another more formal (and faster) option is to use the Heidelberger and Welchâ€™s convergence diagnostic, which is also available in the __coda__ package. In particular, we can ask what percent of the estimates pass the half-width test, which compares the means from beginning and ending sections of the chain.

```{r converge_chk}
## number of estimated params/states
n_par <- dim(mcmc_smpl[[1]])[2]
## H&W diag: pass (1) or fail (NA)
halfwidth_test <- coda::heidel.diag(mcmc_smpl[[1]])[,4]
## proportion passing
round(sum(halfwidth_test, na.rm = TRUE) / n_par, 2)
```

Only about `r round(sum(halfwidth_test, na.rm = TRUE) / n_par, 2)*100`% of the estimates appear to have converged, but that isn't too surprising given the relatively short burnin period and subsequent chain length.

# Examining the fits

(_Note that from here we proceed as though the model had converged_.)

We begin by summarizing the MCMC samples.

```{r examine_fits}
mod_smry <- summary(mcmc_smpl)
par_means <- mod_smry$statistics[,"Mean"]
```

## Loadings matrix

Recall that we constrained $\mathbf{Z}$ in such a way as to choose only one of many possible solutions, but fortunately they are equivalent and can be related to each other by a rotation matrix. Let $\mathbf{H}$ be any $m \times m$ non-singular matrix.  The following are then equivalent DFA models:

$$
\begin{gathered}
 \mathbf{y}_t \sim \text{MVN}(\mathbf{Z} \mathbf{x}_t, \mathbf{R}) \\
 \mathbf{x}_t \sim \text{MVN}(\mathbf{x}_{t-1},\mathbf{Q})
\end{gathered}   
$$

and

$$
\begin{gathered}
 \mathbf{y}_t \sim \text{MVN}(\mathbf{Z} \mathbf{H}^{-1} \mathbf{x}_t, \mathbf{R}) \\
 \mathbf{H}\mathbf{x}_t \sim \text{MVN}(\mathbf{H}\mathbf{x}_{t-1},\mathbf{Q})
\end{gathered}   
$$

There are many ways of doing factor rotations, but a common method is the "varimax"" rotation, which seeks a rotation matrix $\mathbf{H}$ that creates the largest difference between the loadings in $\mathbf{Z}$.
 
The varimax rotation is easy to compute because R has a built in function for this: `varimax()`. Interestingly, the function returns the inverse of $\mathbf{H}$, which we need anyway.  

```{r get_ZZ_fit}
ZZ_fit <- par_means[grepl("ZZ",rownames(mod_smry$statistics))]
ZZ_fit <- matrix(ZZ_fit, NN, MM, byrow=FALSE)
## rotation matrix
HH_inv <- varimax(ZZ_fit)$rotmat
## rotated Z
ZZ_rot <- ZZ_fit %*% HH_inv
round(ZZ_rot, 2)
```

## Factors

Here are the `r MM` factors in $\mathbf{x}$. Note that we need to rotate these just as we did $\mathbf{Z}$. The top panel has the true factors; the bottom panel shows the estimated factors. Note that there is no way to insure that the specific ordering of the estimated factors will match the true factors.

```{r get_factors}
# fitted factors
xx_fit <- par_means[grepl("xx",rownames(mod_smry$statistics))]
xx_fit <- matrix(xx_fit, MM, TT, byrow=FALSE)
## rotated factors
xx_rot <- solve(HH_inv) %*% xx_fit
```

```{r plot_xx_fits, fig.height=7, fig.width=7, echo=FALSE}
par(mfrow=c(2,1), mai=c(0.8,0.8,0.2,0.2))
matplot(t(xx), type="b", lty="solid", cex=0.7,
        xlab="Time", ylab=expression(italic(x)[italic(t)]),
        col=plasma(MM, end=0.8))
matplot(t(xx_rot), type="l", lty="solid", cex=0.7,
        xlab="Time", ylab=expression(italic(x)[italic(t)]),
        col="darkgray")
```

It's difficult to see which factors, if any, align with one another, so here is a graphical representation of their correlation.

```{r corrplot_xx, fig.height=6, fig.width=6}
par(mai=rep(0.1, 4), omi=rep(0.1, 4))
corrplot(cor(t(xx), t(xx_rot)), method="ellipse",
         tl.col = "black", tl.srt = 0, tl.cex = 0.8, tl.offset = 0.7,
         cl.cex = 0.8, cl.offset = 0.9, cl.ratio = 0.2)
```

## Covariance matrix

Here is an estimated of the observation variance.

```{r get_RR_fit}
RR_fit <- par_means[grepl("RR",rownames(mod_smry$statistics))]
round(RR_fit, 2)
```

The estimate is really close to the true value of 0.2.

## Fitted vs observed

Here is a graphical representation of the correlation between the observed and fitted data for the `r NN` time series. Note that their (row, col) ordering is arbitrary.

```{r cor_yy}
## fitted values
yy_fit <- ZZ_rot %*% xx_rot
## corrrelation
cor_yy <- matrix(diag(cor(t(yy), t(yy_fit))), NN/10, 10)
## plots
par(mai=rep(0.1, 4), omi=rep(0.1, 4))
corrplot(cor_yy, method="ellipse",
         tl.pos = "n",
         cl.cex = 0.8, cl.offset = 0.9,
         cl.ratio = 0.2, cl.lim = c(0, 1))
```
